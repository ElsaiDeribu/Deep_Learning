{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7JLiK9yCLj-",
        "outputId": "fb50bd15-d46c-4a68-ac8d-cd16f2b4a55c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forward pass output: [[0.4129139]]\n",
            "Gradient for biases: [array([[-0.06959427]])]\n",
            "Gradient for weights: [array([[-0.03479713, -0.05567541]])]\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"Initialize the network with given layer sizes.\"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Return the output of the network for input `a`.\"\"\"\n",
        "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
        "            a = relu(np.dot(w, a) + b)  # Use ReLU activation for hidden layer\n",
        "        # Output layer with sigmoid activation\n",
        "        a = sigmoid(np.dot(self.weights[-1], a) + self.biases[-1])\n",
        "        return a\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Compute the gradient for the cost function C_x.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # Feedforward\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = relu(z)\n",
        "            activations.append(activation)\n",
        "        # Output layer\n",
        "        z_output = np.dot(self.weights[-1], activation) + self.biases[-1]\n",
        "        activation_output = sigmoid(z_output)\n",
        "        activations.append(activation_output)\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(z_output)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Backpropagation for hidden layers\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = relu_prime(z)  # ReLU derivative\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
        "        return nabla_b, nabla_w\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Return the number of test inputs for which the network outputs the correct result.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Return the vector of partial derivatives ∂C_x / ∂a for the output activations.\"\"\"\n",
        "        return (output_activations - y)\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def relu(z):\n",
        "    \"\"\"ReLU activation function.\"\"\"\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    \"\"\"Derivative of the ReLU function.\"\"\"\n",
        "    return np.where(z > 0, 1, 0)\n",
        "net = Network([2, 1])\n",
        "\n",
        "# Create a sample input\n",
        "input_data = np.array([[0.5], [0.8]])\n",
        "\n",
        "# Perform a forward pass\n",
        "output = net.feedforward(input_data)\n",
        "print(\"Forward pass output:\", output)\n",
        "\n",
        "# Create a sample target output\n",
        "target_output = np.array([[0.7]])\n",
        "\n",
        "# Perform one backpropagation step\n",
        "nabla_b, nabla_w = net.backprop(input_data, target_output)\n",
        "print(\"Gradient for biases:\", nabla_b)\n",
        "print(\"Gradient for weights:\", nabla_w)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
